{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_24152\\1359968282.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 39, in <module>\n",
      "    from pandas.compat import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\compat\\__init__.py\", line 27, in <module>\n",
      "    from pandas.compat.pyarrow import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\compat\\pyarrow.py\", line 8, in <module>\n",
      "    import pyarrow as pa\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_24152\\1359968282.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 9, in <module>\n",
      "    from pandas.core.dtypes.dtypes import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\dtypes\\dtypes.py\", line 24, in <module>\n",
      "    from pandas._libs import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pyarrow\\__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_24152\\1359968282.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\windows_events.py\", line 321, in run_forever\n",
      "    super().run_forever()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3051, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3106, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3311, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3493, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\david\\AppData\\Local\\Temp\\ipykernel_24152\\1359968282.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\david\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, r2_score, silhouette_score, make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('cleaned_apartment_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit a random forest regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']]\n",
    "y = df['price']\n",
    "\n",
    "# One-Hot Encode the 'state' column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_estimators=100))  # Reduced number of trees to speed up\n",
    "])\n",
    "\n",
    "# Define the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Perform cross-validation with 3 folds\n",
    "cv_rmse_scores = cross_val_score(pipeline, X, y, cv=3, scoring=rmse_scorer)\n",
    "print(f'Cross-Validated RMSE: {cv_rmse_scores.mean():.2f} ± {cv_rmse_scores.std():.2f}')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the entire test set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Overall Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'Overall R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The global model has been saved already. So lets open it:\n",
    "# Define features and target variable\n",
    "# X = df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']]\n",
    "# y = df['price']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# pipeline = joblib.load('global_model.pkl')\n",
    "\n",
    "# Transform the test data using the preprocessor within the pipeline\n",
    "# X_test_transformed = pipeline.named_steps['preprocessor'].transform(X_test)\n",
    "\n",
    "# Make predictions on the test set\n",
    "# y_pred = pipeline.named_steps['model'].predict(X_test_transformed)\n",
    "\n",
    "# Evaluate the model on the entire test set\n",
    "# rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "# r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# print(f'Overall Root Mean Squared Error: {rmse:.2f}')\n",
    "# print(f'Overall R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how well the model performs by state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test.copy()\n",
    "results['actual_price'] = y_test\n",
    "results['predicted_price'] = y_pred\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets identify States with low RSME:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is a $200 RMSE threshold reasonable?\n",
    "mean_price = df['price'].mean()\n",
    "relative_rmse_threshold = 200 / mean_price\n",
    "print(f'Relative RMSE Threshold: {relative_rmse_threshold:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A relative RMSE threshold of 14.41% indicates that the RMSE is 14.41% of the mean price. We will use this as a threshold. The error per state should be below that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 200\n",
    "count_threshold = 150\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states = state_performance[(state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "# Filter poor performing states based on RMSE and count threshold\n",
    "poor_states = state_performance[~((state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold))].index.tolist()\n",
    "\n",
    "print(f'Good Performing States: {good_states}')\n",
    "print(f'Poor Performing States: {poor_states}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can continue using the current model for the good performing states and then come up with other models for the remaining states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model = good_states # ['AZ', 'CO', 'KS', 'KY', 'LA', 'MD', 'NE', 'NV', 'OH', 'OK', 'TN', 'TX', 'VA', 'WA', 'HI', 'AL']\n",
    "\n",
    "# Save the pipeline model\n",
    "# joblib.dump(pipeline, 'global_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out good performing states from the DataFrame\n",
    "\n",
    "filtered_df = df[~df['state'].isin(good_states)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to build a model for each of the remaining states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize containers for state models and performance\n",
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "# Define the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Iterate over each poor-performing state to retrain and evaluate the model with CV\n",
    "for state in poor_states:  \n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Define the pipeline for the state\n",
    "    pipeline_state = Pipeline(steps=[\n",
    "        ('model', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    # Perform 3-fold cross-validation\n",
    "    cv_rmse_scores = cross_val_score(pipeline_state, X_state, y_state, cv=3, scoring=rmse_scorer)\n",
    "    mean_cv_rmse = cv_rmse_scores.mean()\n",
    "    std_cv_rmse = cv_rmse_scores.std()\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Train the model on the training data\n",
    "    pipeline_state.fit(X_train_state, y_train_state)\n",
    "    \n",
    "    # Store the model\n",
    "    state_models[state] = pipeline_state\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_state = pipeline_state.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'CV RMSE Mean': mean_cv_rmse,\n",
    "        'CV RMSE Std': std_cv_rmse,\n",
    "        'Test RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - CV RMSE: {mean_cv_rmse:.2f} ± {std_cv_rmse:.2f}, Test RMSE: {rmse_state:.2f}, R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the performance metrics to a DataFrame for better visualization\n",
    "state_performance_df = pd.DataFrame(state_performance).set_index('state')\n",
    "\n",
    "print(\"\\nState Performance Metrics with Cross-Validation:\")\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets get the models for the states that meet the thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "rmse_threshold = 200\n",
    "count_threshold = 150\n",
    "\n",
    "# Filter high-performing states based on the defined thresholds\n",
    "high_performance_states = state_performance_df[\n",
    "    (state_performance_df['CV RMSE Mean'] < rmse_threshold) & \n",
    "    (state_performance_df['Count'] > count_threshold)\n",
    "].index.tolist()\n",
    "\n",
    "print(high_performance_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipelines (that include the models) for high-performing states\n",
    "# for state in high_performance_states:\n",
    "    # pipeline = state_models[state]\n",
    "    # joblib.dump(pipeline, f'{state}_model.pkl')\n",
    "    # print(f'Pipeline for state {state} saved as {state}_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the high-performing states from the DataFrame\n",
    "filtered_df = filtered_df[~filtered_df['state'].isin(high_performance_states)]\n",
    "print(f'\\nRemaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see the value counts for our remaining states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df['state'].nunique())\n",
    "filtered_df['state'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_states = filtered_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "for state in remaining_states:\n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define and train the model\n",
    "    pipeline_state = Pipeline(steps=[\n",
    "        ('model', RandomForestRegressor(random_state=42))\n",
    "    ])\n",
    "    pipeline_state.fit(X_train_state, y_train_state)\n",
    "    \n",
    "    # Store the model\n",
    "    state_models[state] = pipeline_state\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    y_pred_state = pipeline_state.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(state_data)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - RMSE: {rmse_state:.2f}, R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "for state in remaining_states:\n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Perform Grid Search Cross Validation\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the model\n",
    "    state_models[state] = best_model\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_pred_state = best_model.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_test_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - Best Model RMSE: {rmse_state:.2f}, Best Model R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for state-specific performance\n",
    "state_performance_df = pd.DataFrame(state_performance)\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the thresholds\n",
    "rmse_threshold = 200\n",
    "count_threshold = 150 \n",
    "\n",
    "# Filter high-performing states based on the defined thresholds\n",
    "high_performance_states_grid_search = state_performance_df[\n",
    "    (state_performance_df['RMSE'] < rmse_threshold) & \n",
    "    (state_performance_df['Count'] > count_threshold)\n",
    "].index.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_performance_states_df_grid_search = state_performance_df.loc[high_performance_states_grid_search]\n",
    "print(\"\\nDataFrame of High-Performing States:\")\n",
    "print(high_performance_states_df_grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model for NC\n",
    "# state = 'NC'\n",
    "# if state in state_models:\n",
    "    # best_model_nc = state_models[state]\n",
    "    # joblib.dump(best_model_nc, f'{state}_model.pkl')\n",
    "    # print(f'Pipeline for state {state} saved as {state}_model.pkl')\n",
    "# else:\n",
    "    # print(f'Error: State {state} not found in state_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove NC instances from the filtered DataFrame\n",
    "filtered_df = filtered_df[filtered_df['state'] != 'NC']\n",
    "\n",
    "# Check the shape of the remaining DataFrame\n",
    "print(f'\\nRemaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['state'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to fit a Random Forest model to the remaining states df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = filtered_df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']]\n",
    "y = filtered_df['price']\n",
    "\n",
    "# One-Hot Encode the 'state' column\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_estimators=100))  # Reduced number of trees to speed up\n",
    "])\n",
    "\n",
    "# Define the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Perform cross-validation with 3 folds\n",
    "cv_rmse_scores = cross_val_score(pipeline, X, y, cv=3, scoring=rmse_scorer)\n",
    "print(f'Cross-Validated RMSE: {cv_rmse_scores.mean():.2f} ± {cv_rmse_scores.std():.2f}')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the entire test set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Overall Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'Overall R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how it performs by state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test.copy()\n",
    "results['actual_price'] = y_test\n",
    "results['predicted_price'] = y_pred\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 200\n",
    "count_threshold = 150\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model2 = state_performance[(state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model_2 = good_states_global_model2 # ['FL', 'SC']\n",
    "\n",
    "# Save the pipeline model\n",
    "# joblib.dump(pipeline, 'global_model_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out good performing states from the filtered DataFrame\n",
    "\n",
    "filtered_df = filtered_df[~filtered_df['state'].isin(good_states_global_model2)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a SVM for the remaining df:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for SVR\n",
    "param_grid = {\n",
    "    'model__C': [1, 10],  \n",
    "    'model__epsilon': [0.1, 0.2],  \n",
    "    'model__kernel': ['linear', 'rbf']  \n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', SVR())\n",
    "])\n",
    "\n",
    "# Combine data for all remaining states\n",
    "X = filtered_df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "y = filtered_df['price']\n",
    "\n",
    "# Train-test split for the combined dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "global_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the global model on the test set\n",
    "y_pred = global_best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Global Model RMSE: {rmse:.2f}, Global Model R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model for each state\n",
    "state_performance = []\n",
    "remaining_states = filtered_df['state'].unique()\n",
    "\n",
    "for state in remaining_states:\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    y_pred_state = global_best_model.predict(X_state)\n",
    "    rmse_state = mean_squared_error(y_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_state, y_pred_state)\n",
    "    \n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - RMSE: {rmse_state:.2f}, R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the performance metrics to a DataFrame for better visualization\n",
    "state_performance_df = pd.DataFrame(state_performance).set_index('state')\n",
    "\n",
    "print(\"\\nState Performance Metrics for Global SVR Model:\")\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like SVM is not a good option for our goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try Gradient Boosting Regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of boosting stages to be run\n",
    "    'model__learning_rate': [0.01, 0.1],  # Step size at each iteration\n",
    "    'model__max_depth': [3, 5]  # Maximum depth of the individual estimators\n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Combine data for all remaining states\n",
    "X = filtered_df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "y = filtered_df['price']\n",
    "\n",
    "# Train-test split for the combined dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "global_best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the global model on the test set\n",
    "y_pred = global_best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Global Model RMSE: {rmse:.2f}, Global Model R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model for each state\n",
    "state_performance = []\n",
    "remaining_states = filtered_df['state'].unique()\n",
    "\n",
    "for state in remaining_states:\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    y_pred_state = global_best_model.predict(X_state)\n",
    "    rmse_state = mean_squared_error(y_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_state, y_pred_state)\n",
    "    \n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - RMSE: {rmse_state:.2f}, R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the performance metrics to a DataFrame for better visualization\n",
    "state_performance_df = pd.DataFrame(state_performance).set_index('state')\n",
    "\n",
    "print(\"\\nState Performance Metrics for Global Gradient Boosting Regressor:\")\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 200\n",
    "count_threshold = 150\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model3 = state_performance_df[(state_performance_df['RMSE'] < rmse_threshold) & (state_performance_df['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the global best model for 'CT'\n",
    "# joblib.dump(global_best_model, 'CT_model.pkl')\n",
    "# print('Global Gradient Boosting Regressor model saved as CT_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove CT instances from the filtered DataFrame\n",
    "filtered_df = filtered_df[filtered_df['state'] != 'CT']\n",
    "\n",
    "# Check the shape of the remaining DataFrame\n",
    "print(f'\\nRemaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets try to build a Gradient Boosting Regressor for each of the remaining states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Gradient Boosting Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of boosting stages to be run\n",
    "    'model__learning_rate': [0.01, 0.1],  # Step size at each iteration\n",
    "    'model__max_depth': [3, 5]  # Maximum depth of the individual estimators\n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Initialize containers for state models and performance\n",
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "# Get the unique remaining states\n",
    "remaining_states = filtered_df['state'].unique()\n",
    "\n",
    "for state in remaining_states:\n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Perform Grid Search Cross Validation\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the model\n",
    "    state_models[state] = best_model\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_pred_state = best_model.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_test_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - Best Model RMSE: {rmse_state:.2f}, Best Model R-squared: {r2_state:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the performance metrics to a DataFrame for better visualization\n",
    "state_performance_df = pd.DataFrame(state_performance).set_index('state')\n",
    "\n",
    "print(\"\\nState Performance Metrics for Individual Gradient Boosting Regressor Models:\")\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for XGBoost Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of boosting stages to be run\n",
    "    'model__learning_rate': [0.01, 0.1],  # Step size at each iteration\n",
    "    'model__max_depth': [3, 5],  # Maximum depth of the individual estimators\n",
    "    'model__subsample': [0.8, 1.0]  # Subsample ratio of the training instances\n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Initialize containers for state models and performance\n",
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "# Get the unique remaining states\n",
    "remaining_states = filtered_df['state'].unique()\n",
    "\n",
    "for state in remaining_states:\n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Perform Grid Search Cross Validation\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the model\n",
    "    state_models[state] = best_model\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_pred_state = best_model.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_test_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - Best Model RMSE: {rmse_state:.2f}, Best Model R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the performance metrics to a DataFrame for better visualization\n",
    "state_performance_df = pd.DataFrame(state_performance).set_index('state')\n",
    "\n",
    "print(\"\\nState Performance Metrics for Individual XGBoost Regressor Models:\")\n",
    "print(state_performance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a DataFrame with states having less than 1000 rows and then fit a model to this subset to see if this helps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter states with less than 1000 rows\n",
    "states_with_less_than_1000 = filtered_df['state'].value_counts()[filtered_df['state'].value_counts() < 1000].index.tolist()\n",
    "\n",
    "# Create a new DataFrame with these states\n",
    "df_less_than_1000 = filtered_df[filtered_df['state'].isin(states_with_less_than_1000)]\n",
    "\n",
    "print(f\"Shape of DataFrame with states having less than 1000 rows: {df_less_than_1000.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for XGBoost Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of boosting stages to be run\n",
    "    'model__learning_rate': [0.01, 0.1],  # Step size at each iteration\n",
    "    'model__max_depth': [3, 5],  # Maximum depth of the individual estimators\n",
    "    'model__subsample': [0.8, 1.0]  # Subsample ratio of the training instances\n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Combine data for all states with less than 1000 rows\n",
    "X = df_less_than_1000[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "y = df_less_than_1000['price']\n",
    "\n",
    "# Train-test split for the combined dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the global model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Combined Model RMSE: {rmse:.2f}, Combined Model R-squared: {r2:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20, 30],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2, 4]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Combine data for all states with less than 1000 rows\n",
    "X = df_less_than_1000[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "y = df_less_than_1000['price']\n",
    "\n",
    "# Train-test split for the combined dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Combined Model RMSE: {rmse:.2f}, Combined Model R-squared: {r2:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install lightgbm\n",
    "#!pip install catboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGBMRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Define a parameter grid for LightGBM\n",
    "param_grid_lgbm = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__num_leaves': [31, 50],\n",
    "    'model__max_depth': [-1, 10, 20]\n",
    "}\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline_lgbm = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LGBMRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search_lgbm = GridSearchCV(pipeline_lgbm, param_grid_lgbm, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_lgbm.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model_lgbm = grid_search_lgbm.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_lgbm = best_model_lgbm.predict(X_test)\n",
    "rmse_lgbm = mean_squared_error(y_test, y_pred_lgbm, squared=False)\n",
    "r2_lgbm = r2_score(y_test, y_pred_lgbm)\n",
    "\n",
    "print(f'LightGBM Model RMSE: {rmse_lgbm:.2f}, LightGBM Model R-squared: {r2_lgbm:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "\n",
    "# Define a parameter grid for CatBoost\n",
    "param_grid_catboost = {\n",
    "    'model__iterations': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1],\n",
    "    'model__depth': [6, 10]\n",
    "}\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline_catboost = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', CatBoostRegressor(random_state=42, silent=True))\n",
    "])\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search_catboost = GridSearchCV(pipeline_catboost, param_grid_catboost, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_catboost.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model_catboost = grid_search_catboost.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_catboost = best_model_catboost.predict(X_test)\n",
    "rmse_catboost = mean_squared_error(y_test, y_pred_catboost, squared=False)\n",
    "r2_catboost = r2_score(y_test, y_pred_catboost)\n",
    "\n",
    "print(f'CatBoost Model RMSE: {rmse_catboost:.2f}, CatBoost Model R-squared: {r2_catboost:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try building individual models and then have them vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set (without additional feature engineering)\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), features),\n",
    "    ])\n",
    "\n",
    "# Create individual models\n",
    "model1 = RandomForestRegressor(random_state=42)\n",
    "model2 = XGBRegressor(random_state=42)\n",
    "model3 = LGBMRegressor(random_state=42)\n",
    "model4 = Ridge()\n",
    "\n",
    "# Combine models into a voting regressor\n",
    "ensemble_model = VotingRegressor(estimators=[\n",
    "    ('rf', model1), ('xgb', model2), ('lgbm', model3), ('ridge', model4)\n",
    "])\n",
    "\n",
    "# Create a pipeline with a preprocessor and the ensemble model\n",
    "ensemble_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ensemble_model)\n",
    "])\n",
    "\n",
    "# Combine data for all states with less than 1000 rows\n",
    "X = df_less_than_1000[features]\n",
    "y = df_less_than_1000['price']\n",
    "\n",
    "# Train-test split for the combined dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the ensemble model\n",
    "ensemble_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model on the test set\n",
    "y_pred_ensemble = ensemble_pipeline.predict(X_test)\n",
    "rmse_ensemble = mean_squared_error(y_test, y_pred_ensemble, squared=False)\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(f'Ensemble Model RMSE: {rmse_ensemble:.2f}, Ensemble Model R-squared: {r2_ensemble:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try a more redifined Grid Search for California:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for California\n",
    "state = 'CA'\n",
    "state_data = filtered_df[filtered_df['state'] == state]\n",
    "\n",
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']\n",
    "X_state = state_data[features]\n",
    "y_state = state_data['price']\n",
    "\n",
    "# Train-test split for California\n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), features),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20, 30, None],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'California Model RMSE: {rmse_state:.2f}, California Model R-squared: {r2_state:.2f}')\n",
    "\n",
    "# # Output\n",
    "# California Model RMSE: 231.73, California Model R-squared: 0.80\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best model for California so far. Lets save it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model for California\n",
    "# joblib.dump(best_model, 'CA_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data for California\n",
    "state = 'GA'\n",
    "state_data = filtered_df[filtered_df['state'] == state]\n",
    "\n",
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']\n",
    "X_state = state_data[features]\n",
    "y_state = state_data['price']\n",
    "\n",
    "# Train-test split for GA\n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), features),\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20, 30, None],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'GA Model RMSE: {rmse_state:.2f}, GA Model R-squared: {r2_state:.2f}')\n",
    "\n",
    "# # Output\n",
    "# GA Model RMSE: 231.70, GA Model R-squared: 0.60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove CA instances from the filtered DataFrame\n",
    "filtered_df = filtered_df[filtered_df['state'] != 'CA']\n",
    "\n",
    "# Check the shape of the remaining DataFrame\n",
    "print(f'\\nRemaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20, 30, None],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5, 10, 15],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'Model RMSE: {rmse_state:.2f}, Model R-squared: {r2_state:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test_state.copy()\n",
    "results['actual_price'] = y_test_state\n",
    "results['predicted_price'] = y_pred_state\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 220\n",
    "count_threshold = 1\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model_3_1 = state_performance_df[(state_performance_df['RMSE'] < rmse_threshold) & (state_performance_df['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model_3_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model_3_1 = good_states_global_model_3_1 # ['NM', 'WV', 'MO', 'NJ', 'IN', 'SD', 'WY', 'MT']\n",
    "\n",
    "# Save the pipeline model\n",
    "# joblib.dump(best_model, 'global_model_3_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[~filtered_df['state'].isin(good_states_global_model_3_1)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'Model RMSE: {rmse_state:.2f}, Model R-squared: {r2_state:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test_state.copy()\n",
    "results['actual_price'] = y_test_state\n",
    "results['predicted_price'] = y_pred_state\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 225\n",
    "count_threshold = 1\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model_4 = state_performance[(state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model_4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model_4 = good_states_global_model_4 # ['AK', 'MS', 'VT']\n",
    "\n",
    "# Save the pipeline model\n",
    "# joblib.dump(best_model, 'global_model_4.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[~filtered_df['state'].isin(good_states_global_model_4)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'Model RMSE: {rmse_state:.2f}, Model R-squared: {r2_state:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test_state.copy()\n",
    "results['actual_price'] = y_test_state\n",
    "results['predicted_price'] = y_pred_state\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 225\n",
    "count_threshold = 1\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model_5 = state_performance[(state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model_5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model_5 = good_states_global_model_5 # ['DC', 'GA', 'OR']\n",
    "# Save the pipeline model\n",
    "# joblib.dump(best_model, 'global_model_5.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[~filtered_df['state'].isin(good_states_global_model_5)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_states = filtered_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_models = {}\n",
    "state_performance = []\n",
    "\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [10, 20],\n",
    "    'model__min_samples_split': [2, 5],\n",
    "    'model__min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "for state in remaining_states:\n",
    "    # Filter data for the state\n",
    "    state_data = filtered_df[filtered_df['state'] == state]\n",
    "    X_state = state_data[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "    y_state = state_data['price']\n",
    "    \n",
    "    # Train-test split for the state\n",
    "    X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Perform Grid Search Cross Validation\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "    # Get the best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Store the model\n",
    "    state_models[state] = best_model\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    y_pred_state = best_model.predict(X_test_state)\n",
    "    rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "    r2_state = r2_score(y_test_state, y_pred_state)\n",
    "    \n",
    "    # Append performance metrics\n",
    "    state_performance.append({\n",
    "        'state': state,\n",
    "        'RMSE': rmse_state,\n",
    "        'R-squared': r2_state,\n",
    "        'Count': len(y_test_state)\n",
    "    })\n",
    "\n",
    "    print(f'State: {state} - Best Model RMSE: {rmse_state:.2f}, Best Model R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "         ('cat', OneHotEncoder(handle_unknown='ignore'), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with a preprocessor and a model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define a parameter grid for Random Forest Regressor\n",
    "param_grid = {\n",
    "    'model__n_estimators': [100, 200],  # Number of trees in the forest\n",
    "    'model__max_depth': [10, 20],  # Maximum depth of the tree\n",
    "    'model__min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'model__min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "    'model__max_features': ['auto', 'sqrt', 'log2']  # Number of features to consider when looking for the best split\n",
    "}\n",
    "\n",
    "# Perform Grid Search Cross Validation\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Get the best estimator\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "y_pred_state = best_model.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "print(f'Model RMSE: {rmse_state:.2f}, Model R-squared: {r2_state:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with predictions and actual values\n",
    "results = X_test_state.copy()\n",
    "results['actual_price'] = y_test_state\n",
    "results['predicted_price'] = y_pred_state\n",
    "\n",
    "# Analyze performance by state\n",
    "state_performance = results.groupby('state').apply(\n",
    "    lambda df: pd.Series({\n",
    "        'RMSE': mean_squared_error(df['actual_price'], df['predicted_price'], squared=False),\n",
    "        'R-squared': r2_score(df['actual_price'], df['predicted_price']),\n",
    "        'Count': len(df)\n",
    "    })\n",
    ")\n",
    "\n",
    "print(state_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for good performance\n",
    "rmse_threshold = 250\n",
    "count_threshold = 1\n",
    "\n",
    "# Filter good performing states based on RMSE and count threshold\n",
    "good_states_global_model_6 = state_performance[(state_performance['RMSE'] < rmse_threshold) & (state_performance['Count'] > count_threshold)].index.tolist()\n",
    "\n",
    "\n",
    "print(f'Good Performing States: {good_states_global_model_6}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_for_global_model_6 = good_states_global_model_6 # ['ID', 'MA', 'WI']\n",
    "\n",
    "# Save the pipeline model\n",
    "# joblib.dump(best_model, 'global_model_6.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filtered_df[~filtered_df['state'].isin(good_states_global_model_6)]\n",
    "print(f'Remaining States DataFrame shape: {filtered_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df['state'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fit one more model for the remaining states. Lets try different models and choose the one with the best performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline_rf = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Random Forest Regressor\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 20, 30, None],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_rf = GridSearchCV(pipeline_rf, param_grid_rf, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_rf.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_rf = grid_search_rf.best_estimator_\n",
    "y_pred_rf = best_model_rf.predict(X_test_state)\n",
    "rmse_rf = mean_squared_error(y_test_state, y_pred_rf, squared=False)\n",
    "r2_rf = r2_score(y_test_state, y_pred_rf)\n",
    "print(f'Random Forest RMSE: {rmse_rf:.2f}, R-squared: {r2_rf:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire pipeline model\n",
    "# joblib.dump(best_model_gb, 'global_model_7.pkl') # for the remaining states\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional Models: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "pipeline_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for Gradient Boosting Regressor\n",
    "param_grid_gb = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_gb = grid_search_gb.best_estimator_\n",
    "y_pred_gb = best_model_gb.predict(X_test_state)\n",
    "rmse_gb = mean_squared_error(y_test_state, y_pred_gb, squared=False)\n",
    "r2_gb = r2_score(y_test_state, y_pred_gb)\n",
    "print(f'Gradient Boosting RMSE: {rmse_gb:.2f}, R-squared: {r2_gb:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_en = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', ElasticNet(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for ElasticNet\n",
    "param_grid_en = {\n",
    "    'model__alpha': [0.1, 1, 10],\n",
    "    'model__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_en = GridSearchCV(pipeline_en, param_grid_en, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_en.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_en = grid_search_en.best_estimator_\n",
    "y_pred_en = best_model_en.predict(X_test_state)\n",
    "rmse_en = mean_squared_error(y_test_state, y_pred_en, squared=False)\n",
    "r2_en = r2_score(y_test_state, y_pred_en)\n",
    "print(f'ElasticNet RMSE: {rmse_en:.2f}, R-squared: {r2_en:.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_xgb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', XGBRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_xgb = GridSearchCV(pipeline_xgb, param_grid_xgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_xgb.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_xgb = grid_search_xgb.best_estimator_\n",
    "y_pred_xgb = best_model_xgb.predict(X_test_state)\n",
    "rmse_xgb = mean_squared_error(y_test_state, y_pred_xgb, squared=False)\n",
    "r2_xgb = r2_score(y_test_state, y_pred_xgb)\n",
    "print(f'XGBoost RMSE: {rmse_xgb:.2f}, R-squared: {r2_xgb:.2f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gradient Boosting performed well. Lets see if we can futher improve it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the feature set\n",
    "features = ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']\n",
    "X_state = filtered_df[features]\n",
    "y_state = filtered_df['price']\n",
    "\n",
    "# Train-test split \n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X_state, y_state, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline_gb = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', GradientBoostingRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define an extended parameter grid for Gradient Boosting Regressor\n",
    "param_grid_gb = {\n",
    "    'model__n_estimators': [100, 200, 300, 500],\n",
    "    'model__learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'model__max_depth': [3, 5, 7, 9],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4],\n",
    "    'model__subsample': [0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search_gb = GridSearchCV(pipeline_gb, param_grid_gb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "grid_search_gb.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_model_gb = grid_search_gb.best_estimator_\n",
    "y_pred_gb = best_model_gb.predict(X_test_state)\n",
    "rmse_gb = mean_squared_error(y_test_state, y_pred_gb, squared=False)\n",
    "r2_gb = r2_score(y_test_state, y_pred_gb)\n",
    "print(f'Extended Grid Search - Gradient Boosting RMSE: {rmse_gb:.2f}, R-squared: {r2_gb:.2f}')\n",
    "\n",
    "# Print the best parameters\n",
    "print(f'Best parameters found: {grid_search_gb.best_params_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_states = filtered_df['state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the improved pipeline model\n",
    "# joblib.dump(best_model_gb, 'global_model_7_gradient_boosting') \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are done with our models! Lets install Streamlit and create the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am having issues with these states ['IL', 'PA', 'RI', 'MI', 'MN', 'NY', 'DE', 'CT']\n",
    " \n",
    "# Lets try to find a random forest regressor for them:\n",
    "\n",
    "problem_states = ['IL', 'PA', 'RI', 'MI', 'MN', 'NY', 'DE', 'CT']\n",
    "\n",
    "new_df = df[df['state'].isin(problem_states)]\n",
    "\n",
    "# Define features and target variable\n",
    "X = new_df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude', 'state']]\n",
    "y = new_df['price']  \n",
    "\n",
    "# Define the preprocessing for the numerical and categorical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', 'passthrough', ['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']),\n",
    "        ('cat', OneHotEncoder(), ['state'])\n",
    "    ])\n",
    "\n",
    "# Create a pipeline with preprocessing and the Random Forest Regressor\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(random_state=42, n_estimators=100))\n",
    "])\n",
    "\n",
    "# Define the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Perform cross-validation with 3 folds\n",
    "cv_rmse_scores = cross_val_score(pipeline, X, y, cv=3, scoring=rmse_scorer)\n",
    "print(f'Cross-Validated RMSE: {cv_rmse_scores.mean():.2f} ± {cv_rmse_scores.std():.2f}')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the entire test set\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Overall Root Mean Squared Error: {rmse:.2f}')\n",
    "print(f'Overall R-squared: {r2:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the pipeline\n",
    "# pipeline_filename = 'combined_states_pipeline.pkl' # ['IL', 'PA', 'RI', 'MI', 'MN', 'NY', 'DE', 'CT']\n",
    "# joblib.dump(pipeline, pipeline_filename)\n",
    "# print(f'Pipeline saved as {pipeline_filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I am still having issues with 'ME'. Lets build a model for it:\n",
    "\n",
    "state = 'ME'\n",
    "ME_state_df = df[df['state'] == state]\n",
    "ME_state_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target variable\n",
    "X = ME_state_df[['bathrooms', 'bedrooms', 'square_feet', 'latitude', 'longitude']]\n",
    "y = ME_state_df['price']\n",
    "\n",
    "# Define the pipeline for the state\n",
    "pipeline_state = Pipeline(steps=[\n",
    "    ('model', RandomForestRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the scoring metric\n",
    "rmse_scorer = make_scorer(mean_squared_error, squared=False)\n",
    "\n",
    "# Perform 3-fold cross-validation\n",
    "cv_rmse_scores = cross_val_score(pipeline_state, X, y, cv=3, scoring=rmse_scorer)\n",
    "mean_cv_rmse = cv_rmse_scores.mean()\n",
    "std_cv_rmse = cv_rmse_scores.std()\n",
    "\n",
    "# Train-test split for the state\n",
    "X_train_state, X_test_state, y_train_state, y_test_state = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "pipeline_state.fit(X_train_state, y_train_state)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred_state = pipeline_state.predict(X_test_state)\n",
    "rmse_state = mean_squared_error(y_test_state, y_pred_state, squared=False)\n",
    "r2_state = r2_score(y_test_state, y_pred_state)\n",
    "\n",
    "# Store the model\n",
    "model_filename = f'ME_model.pkl'\n",
    "# joblib.dump(pipeline_state, model_filename)\n",
    "print(f'Model for ME saved as {model_filename}')\n",
    "\n",
    "# Append performance metrics\n",
    "state_performance = {\n",
    "    'state': 'ME',\n",
    "    'CV RMSE Mean': mean_cv_rmse,\n",
    "    'CV RMSE Std': std_cv_rmse,\n",
    "    'Test RMSE': rmse_state,\n",
    "    'R-squared': r2_state,\n",
    "    'Count': len(y)\n",
    "}\n",
    "\n",
    "# Print performance metrics\n",
    "print(f'State: ME - CV RMSE: {mean_cv_rmse:.2f} ± {std_cv_rmse:.2f}, Test RMSE: {rmse_state:.2f}, R-squared: {r2_state:.2f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
